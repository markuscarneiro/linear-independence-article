<!DOCTYPE html>
<html lang="pt-BR">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Da Matem√°tica ao ChatGPT: Como a Independ√™ncia Linear Conecta Tudo</title>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/plotly.js/2.18.2/plotly.min.js"></script>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: 'Georgia', 'Times New Roman', serif;
            line-height: 1.8;
            color: #2c3e50;
            background: linear-gradient(135deg, #f5f7fa 0%, #c3cfe2 100%);
            min-height: 100vh;
        }
        
        .article-container {
            max-width: 1000px;
            margin: 0 auto;
            background: white;
            box-shadow: 0 0 30px rgba(0,0,0,0.1);
            border-radius: 10px;
            overflow: hidden;
        }
        
        .header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 60px 40px;
            text-align: center;
        }
        
        .title {
            font-size: 2.8em;
            font-weight: bold;
            margin-bottom: 20px;
            text-shadow: 2px 2px 4px rgba(0,0,0,0.3);
        }
        
        .subtitle {
            font-size: 1.3em;
            opacity: 0.9;
            font-style: italic;
        }
        
        .author-info {
            margin-top: 30px;
            font-size: 1.1em;
            opacity: 0.8;
        }
        
        .content {
            padding: 50px 40px;
        }
        
        .abstract {
            background: #f8f9fa;
            border-left: 5px solid #3498db;
            padding: 30px;
            margin: 40px 0;
            border-radius: 5px;
            font-style: italic;
            font-size: 1.1em;
        }
        
        .abstract h3 {
            margin-bottom: 15px;
            color: #2c3e50;
            font-style: normal;
        }
        
        h2 {
            color: #2c3e50;
            font-size: 1.8em;
            margin: 50px 0 25px 0;
            border-bottom: 3px solid #3498db;
            padding-bottom: 10px;
        }
        
        h3 {
            color: #34495e;
            font-size: 1.4em;
            margin: 35px 0 20px 0;
        }
        
        p {
            margin-bottom: 20px;
            text-align: justify;
            font-size: 1.1em;
        }
        
        .plot-container {
            margin: 40px 0;
            padding: 30px;
            background: #f8f9fa;
            border-radius: 10px;
            border-left: 5px solid #3498db;
        }
        
        .plot-title {
            font-size: 1.2em;
            font-weight: bold;
            color: #2c3e50;
            margin-bottom: 15px;
            text-align: center;
        }
        
        .plot-description {
            background: #e8f4fd;
            padding: 15px;
            border-radius: 8px;
            margin: 15px 0;
            font-size: 1em;
            border-left: 4px solid #2196f3;
        }
        
        .code-block {
            background: #2c3e50;
            color: #ecf0f1;
            padding: 20px;
            border-radius: 8px;
            font-family: 'Courier New', monospace;
            margin: 20px 0;
            overflow-x: auto;
            font-size: 0.95em;
        }
        
        .highlight-box {
            background: linear-gradient(135deg, #fff3e0, #ffe0b2);
            padding: 20px;
            border-radius: 8px;
            border-left: 5px solid #ff9800;
            margin: 25px 0;
            font-weight: 500;
        }
        
        .equation {
            background: #f8f9fa;
            padding: 15px;
            border-radius: 5px;
            text-align: center;
            font-family: 'Times New Roman', serif;
            font-size: 1.1em;
            margin: 20px 0;
            border: 1px solid #dee2e6;
        }
        
        .grid-2 {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 30px;
            margin: 30px 0;
        }
        
        .conclusion {
            background: linear-gradient(135deg, #667eea, #764ba2);
            color: white;
            padding: 40px;
            margin: 50px 0 0 0;
            border-radius: 10px;
        }
        
        .conclusion h2 {
            color: white;
            border-bottom: 3px solid rgba(255,255,255,0.3);
        }
        
        .footer {
            background: #2c3e50;
            color: white;
            padding: 30px;
            text-align: center;
            font-style: italic;
        }
        
        @media (max-width: 768px) {
            .grid-2 { grid-template-columns: 1fr; }
            .content { padding: 30px 20px; }
            .header { padding: 40px 20px; }
            .title { font-size: 2.2em; }
            .subtitle { font-size: 1.1em; }
        }
        
        /* Estilo para listas */
        ul, ol {
            margin: 20px 0;
            padding-left: 30px;
        }
        
        li {
            margin-bottom: 10px;
        }
        
        /* Estilo para cita√ß√µes */
        blockquote {
            border-left: 4px solid #3498db;
            padding-left: 20px;
            margin: 20px 0;
            font-style: italic;
            color: #555;
        }
    </style>
</head>
<body>
    <div class="article-container">
        <!-- HEADER -->
        <div class="header">
            <h1 class="title">Da Matem√°tica ao ChatGPT</h1>
            <p class="subtitle">Como a Independ√™ncia Linear Conecta Tudo</p>
            <div class="author-info">
                Um estudo sobre as conex√µes fundamentais entre √°lgebra linear e intelig√™ncia artificial moderna
            </div>
        </div>
        
        <!-- CONTE√öDO PRINCIPAL -->
        <div class="content">
            <!-- ABSTRACT -->
            <div class="abstract">
                <h3>üéØ Resumo</h3>
                <p>Este artigo explora como o conceito fundamental de independ√™ncia linear na matem√°tica conecta √°reas aparentemente distintas: desde a detec√ß√£o de multicolinearidade em ci√™ncia de dados at√© os mecanismos de positional encoding que permitem ao ChatGPT compreender a ordem das palavras. Atrav√©s de visualiza√ß√µes interativas e exemplos pr√°ticos, demonstramos que a mesma matem√°tica que governa a limpeza de dados redundantes tamb√©m fundamenta os avan√ßos mais espetaculares da intelig√™ncia artificial moderna.</p>
            </div>
            
            <!-- INTRODU√á√ÉO -->
            <h2>üöÄ Introdu√ß√£o: Uma Jornada Inesperada</h2>
            
            <p>Imagine descobrir que o mesmo conceito matem√°tico que explica por que algumas vari√°veis s√£o redundantes em seus dados tamb√©m √© a base de como o ChatGPT entende a ordem das palavras. Esta √© a hist√≥ria da <strong>independ√™ncia linear</strong> - um conceito que parece abstrato, mas que secretamente governa desde a detec√ß√£o de fraudes at√© a revolu√ß√£o dos transformers.</p>
            
            <p>Em nossa era de big data e intelig√™ncia artificial, frequentemente tratamos a matem√°tica como uma ferramenta isolada, aplicada pontualmente para resolver problemas espec√≠ficos. Por√©m, uma an√°lise mais profunda revela conex√µes surpreendentes entre conceitos fundamentais e suas aplica√ß√µes revolucion√°rias.</p>
            
            <!-- CAP√çTULO 1 -->
            <h2>üìä 1. O Conceito Fundamental</h2>
            
            <h3>O Que √â Independ√™ncia Linear?</h3>
            
            <p>Um subespa√ßo vetorial √© fundamentalmente um conjunto de vetores que satisfaz tr√™s condi√ß√µes essenciais:</p>
            
            <ol>
                <li><strong>Fechamento sob adi√ß√£o:</strong> A soma de dois vetores do subespa√ßo permanece no subespa√ßo</li>
                <li><strong>Fechamento sob multiplica√ß√£o escalar:</strong> Multiplicar um vetor por um n√∫mero mant√©m o resultado no subespa√ßo</li>
                <li><strong>Presen√ßa do vetor nulo:</strong> O vetor zero sempre est√° presente</li>
            </ol>
            
            <p>Mas aqui est√° a revela√ß√£o surpreendente: <strong>fun√ß√µes tamb√©m podem ser vetores</strong>. Sin(x) e cos(x) n√£o s√£o apenas curvas no plano - s√£o vetores em um espa√ßo de dimens√£o infinita chamado espa√ßo funcional.</p>
            
            <h3>O Teste Decisivo</h3>
            
            <p>Duas fun√ß√µes (ou vari√°veis) s√£o <strong>linearmente independentes</strong> se n√£o conseguimos encontrar uma combina√ß√£o linear que resulte em zero para todos os valores. Em outras palavras:</p>
            
            <blockquote>
                <em>"N√£o existe uma 'receita fixa' que fa√ßa essas vari√°veis se cancelarem completamente"</em>
            </blockquote>
            
            <!-- FIGURA 1 -->
            <div class="plot-container">
                <div class="plot-title">Figura 1: Independ√™ncia vs Depend√™ncia Linear</div>
                <div class="grid-2">
                    <div>
                        <div id="plot1a"></div>
                        <div class="plot-description">
                            <strong>‚úÖ Independentes:</strong> sin(x) e cos(x) - A √°rea sombreada tem "volume" e n√£o passa sempre por zero.
                        </div>
                    </div>
                    <div>
                        <div id="plot1b"></div>
                        <div class="plot-description">
                            <strong>‚ùå Dependentes:</strong> sin(x) e -sin(x) - A combina√ß√£o sempre resulta em zero.
                        </div>
                    </div>
                </div>
            </div>
            
            <p>Quando plotamos sin(x) e cos(x), vemos que suas combina√ß√µes lineares criam uma √°rea sombreada que <strong>n√£o passa sempre por zero</strong>. Isso √© a prova visual de que s√£o independentes - cada uma carrega informa√ß√£o √∫nica e irredut√≠vel.</p>
            
            <!-- CAP√çTULO 2 -->
            <h2>üîç 2. A Conex√£o com Ci√™ncia de Dados</h2>
            
            <h3>Multicolinearidade: O Inimigo Oculto</h3>
            
            <p>Na detec√ß√£o de fraudes, imagine que voc√™ tem estas features:</p>
            
            <ul>
                <li><code>valor_transacao</code>: R$ 500</li>
                <li><code>valor_em_dolares</code>: $100 (mesmo valor convertido)</li>
                <li><code>valor_multiplicado_por_2</code>: R$ 1000</li>
            </ul>
            
            <p>Estas vari√°veis s√£o <strong>linearmente dependentes</strong> - uma √© sempre uma transforma√ß√£o matem√°tica das outras. O problema? Seu modelo de machine learning acha que o valor da transa√ß√£o √© 3x mais importante do que realmente √©, porque a mesma informa√ß√£o aparece disfar√ßada tr√™s vezes.</p>
            
            <!-- FIGURA 2 -->
            <div class="plot-container">
                <div class="plot-title">Figura 2: Multicolinearidade em Dados Reais</div>
                <div id="plot2"></div>
                <div class="code-block">
# Exemplo de features com multicolinearidade
valor_brl = [100, 200, 300, 400, 500]
valor_usd = [valor/5 for valor in valor_brl]  # Sempre BRL/5
valor_2x = [valor*2 for valor in valor_brl]   # Sempre BRL*2

# Correla√ß√£o perfeita = depend√™ncia linear!
</div>
                <div class="plot-description">
                    Esta visualiza√ß√£o demonstra como vari√°veis dependentes criam redund√¢ncia nos dados, levando a instabilidade nos modelos de machine learning.
                </div>
            </div>
            
            <h3>A Solu√ß√£o Matem√°tica</h3>
            
            <p>A independ√™ncia linear nos ensina a identificar e remover essa redund√¢ncia:</p>
            
            <div class="code-block">
# Detecta multicolinearidade (depend√™ncia linear)
correlation_matrix = df.corr()
# Se correla√ß√£o > 0.9, s√£o quase linearmente dependentes
# Remove uma delas antes de treinar
</div>
            
            <p>O resultado? Modelos mais est√°veis, interpret√°veis e que generalizam melhor.</p>
            
            <!-- CAP√çTULO 3 -->
            <h2>üåê 3. Visualizando o Invis√≠vel</h2>
            
            <h3>O Desafio das M√∫ltiplas Dimens√µes</h3>
            
            <p>Quando temos 2 features, nossos dados "vivem" em R¬≤ (um plano). Com 3 features, em R¬≥ (um volume). Mas e com 100 features? Nossos dados existem em R¬π‚Å∞‚Å∞ - um hiperespa√ßo que n√£o conseguimos visualizar diretamente.</p>
            
            <p>A dimens√£o de um espa√ßo √© determinada pelo <strong>n√∫mero de features linearmente independentes</strong>. Se voc√™ tem 10 features mas 3 s√£o redundantes, seus dados realmente vivem em R‚Å∑, n√£o R¬π‚Å∞.</p>
            
            <!-- FIGURA 3 -->
            <div class="plot-container">
                <div class="plot-title">Figura 3: Visualizando Alta Dimensionalidade</div>
                <div class="grid-2">
                    <div>
                        <div id="plot3a"></div>
                        <div class="plot-description">
                            <strong>Proje√ß√£o 3D ‚Üí 2D:</strong> Perdemos informa√ß√£o, mas mantemos a estrutura essencial dos dados.
                        </div>
                    </div>
                    <div>
                        <div id="plot3b"></div>
                        <div class="plot-description">
                            <strong>Matriz de Correla√ß√£o:</strong> Visualiza depend√™ncias entre todas as vari√°veis simultaneamente.
                        </div>
                    </div>
                </div>
            </div>
            
            <h3>Estrat√©gias para "Ver" o Invis√≠vel</h3>
            
            <p>Como visualizamos alta dimensionalidade? Atrav√©s de truques matem√°ticos:</p>
            
            <ul>
                <li><strong>PCA:</strong> Projeta 100 dimens√µes em 2D mantendo a informa√ß√£o essencial</li>
                <li><strong>t-SNE:</strong> Preserva rela√ß√µes de proximidade em dimens√µes menores</li>
                <li><strong>Gr√°ficos de correla√ß√£o:</strong> Mostram depend√™ncias entre todas as vari√°veis</li>
            </ul>
            
            <p>√â como analisar sombras de um objeto 4D para entender sua forma real.</p>
            
            <!-- CAP√çTULO 4 -->
            <h2>üåä 4. Por Que Sin e Cos S√£o Especiais?</h2>
            
            <h3>Mais Que Pedagogia</h3>
            
            <p>Sin(x) e cos(x) n√£o s√£o escolhas did√°ticas aleat√≥rias. S√£o as <strong>fun√ß√µes mais fundamentais</strong> para representar padr√µes c√≠clicos na natureza:</p>
            
            <ul>
                <li><strong>M√∫sica:</strong> MP3 usa Transformada de Fourier (decomposi√ß√£o em sin/cos)</li>
                <li><strong>Imagens:</strong> JPEG comprime usando componentes trigonom√©tricas</li>
                <li><strong>S√©ries temporais:</strong> Sazonalidade em vendas segue padr√µes sin/cos</li>
                <li><strong>Engenharia de features:</strong> Capturar ciclicidade temporal</li>
            </ul>
            
            <!-- FIGURA 4 -->
            <div class="plot-container">
                <div class="plot-title">Figura 4: Propriedades Especiais das Fun√ß√µes Trigonom√©tricas</div>
                <div class="grid-2">
                    <div>
                        <div id="plot4a"></div>
                        <div class="plot-description">
                            <strong>Periodicidade:</strong> Sin e cos capturam padr√µes c√≠clicos naturais - essencial para dados temporais.
                        </div>
                    </div>
                    <div>
                        <div id="plot4b"></div>
                        <div class="plot-description">
                            <strong>Feature Engineering:</strong> Transforma√ß√£o de hora linear em features c√≠clicas.
                        </div>
                    </div>
                </div>
            </div>
            
            <div class="code-block">
# Transformar hora em features c√≠clicas
df['hora_sin'] = np.sin(2 * np.pi * df['hora'] / 24)
df['hora_cos'] = np.cos(2 * np.pi * df['hora'] / 24)
# Agora 23h est√° matematicamente "perto" de 1h
</div>
            
            <h3>A Base Universal</h3>
            
            <p>Sin e cos s√£o especiais porque:</p>
            
            <ul>
                <li><strong>Limitadas:</strong> Sempre entre -1 e 1</li>
                <li><strong>Peri√≥dicas:</strong> Capturam ciclos naturais</li>
                <li><strong>Suaves:</strong> Matematicamente bem comportadas</li>
                <li><strong>Ortogonais:</strong> Perfeitamente independentes</li>
            </ul>
            
            <!-- CAP√çTULO 5 -->
            <h2>ü§ñ 5. A Revolu√ß√£o dos Transformers</h2>
            
            <h3>O Problema da Ordem</h3>
            
            <p>Aqui est√° onde tudo se conecta de forma espetacular. O paper "Attention is All You Need" enfrentava um problema fundamental: como fazer o modelo entender que "O gato subiu no telhado" √© diferente de "Telhado no subiu gato o"?</p>
            
            <h3>A Solu√ß√£o Trigonom√©trica</h3>
            
            <p>A resposta estava na independ√™ncia linear de sin e cos:</p>
            
            <div class="equation">
                PE(pos, 2i) = sin(pos / 10000^(2i/d_model))<br>
                PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))
            </div>
            
            <!-- FIGURA 5 -->
            <div class="plot-container">
                <div class="plot-title">Figura 5: Positional Encoding nos Transformers</div>
                <div id="plot5"></div>
                <div class="code-block">
# Positional Encoding do Transformer
PE(pos, 2i) = sin(pos / 10000^(2i/d_model))
PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))

# Cada posi√ß√£o tem uma "assinatura" trigonom√©trica √∫nica!
</div>
                <div class="plot-description">
                    <strong>üéØ A Conex√£o:</strong> Diferentes frequ√™ncias s√£o linearmente independentes, permitindo que o modelo capture padr√µes de curta e longa dist√¢ncia simultaneamente.
                </div>
            </div>
            
            <p>Cada posi√ß√£o no texto recebe uma "assinatura" √∫nica baseada em m√∫ltiplas frequ√™ncias trigonom√©tricas. Diferentes frequ√™ncias (1, 1/100, 1/10000) s√£o <strong>linearmente independentes</strong>, permitindo que o modelo capture padr√µes de curta e longa dist√¢ncia simultaneamente.</p>
            
            <h3>Por Que Funciona</h3>
            
            <ol>
                <li><strong>Determin√≠stico:</strong> Mesma posi√ß√£o = mesma encoding sempre</li>
                <li><strong>Suave:</strong> Posi√ß√µes pr√≥ximas t√™m encodings similares</li>
                <li><strong>Extrapol√°vel:</strong> Funciona para textos maiores que os do treinamento</li>
                <li><strong>Eficiente:</strong> N√£o precisa treinar esses par√¢metros</li>
            </ol>
            
            <div class="highlight-box">
                <strong>üöÄ Revela√ß√£o:</strong> O ChatGPT entende ordem das palavras porque sin e cos s√£o linearmente independentes!
            </div>
            
            <!-- FIGURA 6 -->
            <div class="plot-container">
                <div class="plot-title">Figura 6: A Grande Conex√£o - S√≠ntese dos Conceitos</div>
                <div id="plot6"></div>
                <div class="plot-description">
                    <strong>üîó S√≠ntese:</strong> O mesmo conceito matem√°tico governa desde limpeza de dados at√© a revolu√ß√£o da IA moderna. Diferentes componentes de frequ√™ncia (sin/cos) permitem representar padr√µes complexos de forma independente e eficiente.
                </div>
            </div>
            
            <!-- CONCLUS√ÉO -->
            <div class="conclusion">
                <h2>üéØ Conclus√£o: A Eleg√¢ncia da Matem√°tica</h2>
                
                <p>Esta jornada revela uma verdade profunda: <strong>conceitos matem√°ticos fundamentais conectam √°reas aparentemente distintas</strong>. A mesma independ√™ncia linear que:</p>
                
                <ul>
                    <li>Nos ajuda a limpar dados redundantes</li>
                    <li>Explica por que algumas vari√°veis s√£o √∫teis e outras n√£o</li>
                    <li>Permite visualizar espa√ßos de alta dimens√£o</li>
                    <li>Fundamenta o processamento de sinais</li>
                </ul>
                
                <p>...tamb√©m √© o que permite aos modelos de linguagem mais avan√ßados do mundo entenderem a ordem das palavras e gerarem texto coerente.</p>
                
                <p>Quando voc√™ remove multicolinearidade de seus dados, est√° aplicando a mesma matem√°tica que permite ao GPT-4 escrever este texto. Quando visualiza correla√ß√µes em um heatmap, usa os princ√≠pios que governam como transformers processam linguagem natural.</p>
                
                <p><strong>A matem√°tica n√£o √© apenas uma ferramenta</strong> - √© a linguagem universal que conecta problemas pr√°ticos de ci√™ncia de dados com os avan√ßos mais espetaculares da intelig√™ncia artificial moderna.</p>
                
                <p>Da pr√≥xima vez que detectar multicolinearidade em seus dados, lembre-se: voc√™ est√° tocando nos mesmos princ√≠pios matem√°ticos que fazem o ChatGPT funcionar. √â pura eleg√¢ncia matem√°tica em a√ß√£o.</p>
            </div>
        </div>
        
        <!-- FOOTER -->
        <div class="footer">
            <p><em>"Em matem√°tica, n√£o existem coincid√™ncias - apenas conex√µes esperando para serem descobertas."</em></p>
        </div>
    </div>

    <script>
        // Configura√ß√£o padr√£o para todos os gr√°ficos
        const defaultLayout = {
            font: { family: 'Georgia, serif', size: 12 },
            paper_bgcolor: 'rgba(0,0,0,0)',
            plot_bgcolor: 'rgba(248,249,250,1)',
            margin: { t: 40, r: 20, b: 50, l: 50 }
        };

        // Dados base
        const x = [];
        for (let i = -6; i <= 6; i += 0.2) { x.push(i); }
        const sinx = x.map(val => Math.sin(val));
        const cosx = x.map(val => Math.cos(val));

        // 1A: Independentes
        const combination = x.map((val, i) => sinx[i] + cosx[i]);
        
        Plotly.newPlot('plot1a', [
            {
                x: x, y: sinx, name: 'sin(x)', type: 'scatter', mode: 'lines',
                line: {color: '#3498db', width: 3}
            },
            {
                x: x, y: cosx, name: 'cos(x)', type: 'scatter', mode: 'lines',
                line: {color: '#e74c3c', width: 3}
            },
            {
                x: x, y: combination, name: 'sin(x) + cos(x)', type: 'scatter', mode: 'lines',
                fill: 'tonexty', fillcolor: 'rgba(46, 204, 113, 0.3)',
                line: {color: '#2ecc71', width: 2}
            }
        ], {
            ...defaultLayout,
            title: 'Vari√°veis Independentes',
            xaxis: {title: 'x'},
            yaxis: {title: 'Valor'},
            height: 400
        });

        // 1B: Dependentes
        const minussinx = sinx.map(val => -val);
        const zero = x.map(() => 0);

        Plotly.newPlot('plot1b', [
            {
                x: x, y: sinx, name: 'sin(x)', type: 'scatter', mode: 'lines',
                line: {color: '#3498db', width: 3}
            },
            {
                x: x, y: minussinx, name: '-sin(x)', type: 'scatter', mode: 'lines',
                line: {color: '#e74c3c', width: 3}
            },
            {
                x: x, y: zero, name: 'sin(x) + (-sin(x)) = 0', type: 'scatter', mode: 'lines',
                line: {color: '#2c3e50', width: 4, dash: 'dash'}
            }
        ], {
            ...defaultLayout,
            title: 'Vari√°veis Dependentes',
            xaxis: {title: 'x'},
            yaxis: {title: 'Valor'},
            height: 400
        });

        // 2: Multicolinearidade
        const valores_brl = [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000];
        const valores_usd = valores_brl.map(v => v/5);
        const valores_2x = valores_brl.map(v => v*2);

        Plotly.newPlot('plot2', [
            {
                x: valores_brl, y: valores_usd, name: 'Valor USD (BRL/5)', 
                type: 'scatter', mode: 'markers+lines',
                marker: {size: 10, color: '#3498db'},
                line: {width: 3}
            },
            {
                x: valores_brl, y: valores_2x, name: 'Valor 2x (BRL√ó2)', 
                type: 'scatter', mode: 'markers+lines', yaxis: 'y2',
                marker: {size: 10, color: '#e74c3c'},
                line: {width: 3}
            }
        ], {
            ...defaultLayout,
            title: 'Exemplo de Multicolinearidade em Dados Reais',
            xaxis: {title: 'Valor BRL (Original)'},
            yaxis: {title: 'Valor USD', side: 'left'},
            yaxis2: {title: 'Valor 2x', side: 'right', overlaying: 'y'},
            height: 450
        });

        // 3A: Proje√ß√£o 3D para 2D
        const n = 50;
        const x3d = [], y3d = [], z3d = [];
        for (let i = 0; i < n; i++) {
            const t = (i / n) * 4 * Math.PI;
            x3d.push(Math.cos(t));
            y3d.push(Math.sin(t));
            z3d.push(t * 0.1);
        }

        Plotly.newPlot('plot3a', [
            {
                x: x3d, y: y3d, type: 'scatter', mode: 'markers+lines',
                marker: {size: 8, color: z3d, colorscale: 'Viridis', showscale: true},
                line: {width: 3}
            }
        ], {
            ...defaultLayout,
            title: 'Proje√ß√£o 3D ‚Üí 2D',
            xaxis: {title: 'Dimens√£o 1'},
            yaxis: {title: 'Dimens√£o 2'},
            height: 400
        });

        // 3B: Matriz de Correla√ß√£o
        const corrMatrix = [
            [1.0, 0.1, 0.95],
            [0.1, 1.0, 0.05],
            [0.95, 0.05, 1.0]
        ];

        Plotly.newPlot('plot3b', [{
            z: corrMatrix,
            type: 'heatmap',
            colorscale: [
                [0, '#3498db'], [0.5, '#ecf0f1'], [1, '#e74c3c']
            ],
            showscale: true
        }], {
            ...defaultLayout,